{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU_f7FaP768v",
        "outputId": "cabd09a3-73aa-49d6-a539-f52ca3783084"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "tbMqFT2P7QpG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnP5LXNc7tqi",
        "outputId": "90a3a324-8f0b-44a0-97a0-e3a1890034ef"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess your dataset (replace 'your_dataset.csv' with your data)\n",
        "data = pd.read_csv('/content/data_with_predictions.csv')\n",
        "data = data.dropna()\n",
        "texts = data['commentaire'].values\n",
        "labels = data['predicted_problems'].values  # Replace 'label' with the name of your label column\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n"
      ],
      "metadata": {
        "id": "-gu1JEkB7v4k"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16  # or 32\n",
        "MAX_LENGTH = 128  # Reduce the value\n"
      ],
      "metadata": {
        "id": "fr7aXN85DIbz"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad the text data\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for text in texts:\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "input_ids = np.array(input_ids)\n",
        "attention_masks = np.array(attention_masks)"
      ],
      "metadata": {
        "id": "GXrF4lVO7wDE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    input_ids,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "0XI6TX5Z7wJu"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "O2HBnKTBCjlB"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9mxPg9rC0YL"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "input_ids_input = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='input_ids')\n",
        "attention_masks_input = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
        "bert_output = model(input_ids_input, attention_mask=attention_masks_input)[0]\n",
        "output = tf.keras.layers.Dense(6, activation='softmax')(bert_output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids_input, attention_masks_input], outputs=output)\n"
      ],
      "metadata": {
        "id": "XFelFG_57wMk"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler le modèle\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "F1j9OKCA7wPV"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train, X_train],  # Input data\n",
        "    y_train,             # Labels\n",
        "    epochs=4,       # Number of training epochs\n",
        "    batch_size=16,  # Batch size\n",
        "    validation_data=([X_test, X_test], y_test),  # Validation data\n",
        "    verbose=1            # Verbosity mode\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfMED4FI7wSE",
        "outputId": "c3f45bd4-647f-49f5-e43f-5f2721d48001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "306/532 [================>.............] - ETA: 1:27 - loss: nan - accuracy: 2.0425e-04"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPYAeGLn7wUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate([X_test, X_test], y_test, verbose=0)\n",
        "print(f'Test loss: {loss:.4f}')\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJaTeMsU7wXH",
        "outputId": "94421c93-4f03-4999-edc5-ae394b9dd363"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: nan\n",
            "Test accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your comment\n",
        "new_comment = \"Trop mauvais service,trop de bruit.\"\n",
        "\n",
        "# Tokenize and pad the comment\n",
        "encoded_comment = tokenizer.encode_plus(\n",
        "    new_comment,\n",
        "    add_special_tokens=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    padding='max_length',\n",
        "    return_attention_mask=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "input_ids_comment = np.array([encoded_comment['input_ids']])\n",
        "attention_masks_comment = np.array([encoded_comment['attention_mask']])\n",
        "# Make predictions\n",
        "predictions = model.predict([input_ids_comment, attention_masks_comment])\n",
        "\n",
        "# Convert the predictions to class indices\n",
        "predicted_class_index = np.argmax(predictions, axis=1)\n",
        "predicted_class_label = le.inverse_transform(predicted_class_index)[0]\n",
        "print(\"Comment:\", new_comment)\n",
        "print(\"Predicted Class Label:\", predicted_class_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp2lfIap7wZb",
        "outputId": "ec07a618-d894-4ae2-b83a-ce0b94d3a476"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "Comment: Trop mauvais service,trop de bruit.\n",
            "Predicted Class Label: [\"Problèmes d'Équipements et d'Installations\", 'Bruits', 'Problèmes de Prix', 'Retard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSVxMS-27wct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQ8ugrb97w1z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}